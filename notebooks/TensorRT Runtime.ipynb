{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorRT Runtime\n",
    "\n",
    "This example walks through the basic usecase of:\n",
    "  1. initialization the infer-runtime\n",
    "  2. loading a model\n",
    "  3. allocating resources\n",
    "  4. inspecting the input/output bindings of the model\n",
    "  5. evaluating the model using async futures\n",
    "  6. testing for correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import wurlitzer\n",
    "\n",
    "import infer\n",
    "import infer_test_utils as utils\n",
    "\n",
    "# this allows us to capture stdout and stderr from the backend c++ infer-runtime\n",
    "display_output = wurlitzer.sys_pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec # trtexec --onnx=/work/models/onnx/mnist-v1.3/model.onnx --saveEngine=/work/models/onnx/mnist-v1.3/mnist-v1.3.engine\n",
      "[I] onnx: /work/models/onnx/mnist-v1.3/model.onnx\n",
      "[I] saveEngine: /work/models/onnx/mnist-v1.3/mnist-v1.3.engine\n",
      "----------------------------------------------------------------\n",
      "Input filename:   /work/models/onnx/mnist-v1.3/model.onnx\n",
      "ONNX IR version:  0.0.3\n",
      "Opset version:    8\n",
      "Producer name:    CNTK\n",
      "Producer version: 2.5.1\n",
      "Domain:           ai.cntk\n",
      "Model version:    1\n",
      "Doc string:       \n",
      "----------------------------------------------------------------\n",
      "[I] [TRT] Detected 1 input and 1 output network tensors.\n",
      "[I] Engine has been successfully saved to /work/models/onnx/mnist-v1.3/mnist-v1.3.engine\n",
      "[I] name= Input3, bindingIndex=0, buffers.size()=2\n",
      "[I] name= Plus214_Output_0, bindingIndex=1, buffers.size()=2\n",
      "[I] Average over 10 runs is 0.0651264 ms (host walltime is 0.107141 ms, 99% percentile time is 0.091136).\n",
      "[I] Average over 10 runs is 0.0608256 ms (host walltime is 0.103748 ms, 99% percentile time is 0.064512).\n",
      "[I] Average over 10 runs is 0.0610304 ms (host walltime is 0.103952 ms, 99% percentile time is 0.062464).\n",
      "[I] Average over 10 runs is 0.0606208 ms (host walltime is 0.103556 ms, 99% percentile time is 0.063488).\n",
      "[I] Average over 10 runs is 0.0608256 ms (host walltime is 0.101731 ms, 99% percentile time is 0.063488).\n",
      "[I] Average over 10 runs is 0.0605184 ms (host walltime is 0.10369 ms, 99% percentile time is 0.063488).\n",
      "[I] Average over 10 runs is 0.0607232 ms (host walltime is 0.103788 ms, 99% percentile time is 0.06144).\n",
      "[I] Average over 10 runs is 0.0611328 ms (host walltime is 0.101827 ms, 99% percentile time is 0.063488).\n",
      "[I] Average over 10 runs is 0.0607232 ms (host walltime is 0.103715 ms, 99% percentile time is 0.062464).\n",
      "[I] Average over 10 runs is 0.0606208 ms (host walltime is 0.105943 ms, 99% percentile time is 0.062464).\n",
      "&&&& PASSED TensorRT.trtexec # trtexec --onnx=/work/models/onnx/mnist-v1.3/model.onnx --saveEngine=/work/models/onnx/mnist-v1.3/mnist-v1.3.engine\n"
     ]
    }
   ],
   "source": [
    "!trtexec --onnx=/work/models/onnx/mnist-v1.3/model.onnx --saveEngine=/work/models/onnx/mnist-v1.3/mnist-v1.3.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize infer-runtime\n",
    "\n",
    "The most important option when initializing the infer-runtime is to set the maximum number of conncurrent executions that can be executed at any given time.  This value is tunable for your application.  Lower setting reduce latency; higher-settings increase throughput.  Evaluate how your model performs using ...TODO-this-notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I0108 18:03:07.392199  2869 inference_manager.cc:64] -- Initialzing TensorRT Resource Manager --\n",
      "I0108 18:03:07.392215  2869 inference_manager.cc:65] Maximum Execution Concurrency: 2\n",
      "I0108 18:03:07.392217  2869 inference_manager.cc:66] Maximum Copy Concurrency: 5\n"
     ]
    }
   ],
   "source": [
    "with display_output():\n",
    "    models = infer.InferenceManager(max_executions=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Register a Model\n",
    "\n",
    "To register a model, simply associate a `model_name` with a path to a TensorRT engine file. The returned object is an `InferRunner` object.  Use an `InferRunner` to submit work to the backend inference queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with display_output():\n",
    "    mnist = models.register_tensorrt_engine(\"mnist\", \"/work/models/onnx/mnist-v1.3/mnist-v1.3.engine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Allocate Resources\n",
    "\n",
    "Before you can submit inference requests, you need to allocate some internal resources.  This should be done anytime new models are registered.  There maybe a runtime performance interruption if you update the resources while the queue is full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'display_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bd465fa5fbd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mdisplay_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display_output' is not defined"
     ]
    }
   ],
   "source": [
    "with display_output():\n",
    "    models.update_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Model\n",
    "\n",
    "Query the `InferenceRunner` to see what it expects for inputs and what it will return for outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Input3': {'dtype': dtype('float32'), 'shape': [1, 28, 28]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.input_bindings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Plus214_Output_0': {'dtype': dtype('float32'), 'shape': [10]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.output_bindings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Submit Infer Requests\n",
    "\n",
    "`InferenceRunner.infer` accecpts a dict of numpy arrays that match the input description, submits this inference request to the backend compute engine and returns a future to a dict of numpy arrays.  \n",
    "\n",
    "That means, this method should returns almost immediately; however, that does not mean the inference is complete.  Use `get()` to wait for the result.  This is a blocking call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<infer.InferFuture at 0x7f9fb1344110>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = mnist.infer(Input3=np.random.random_sample([1,28,28]))\n",
    "result # result is a future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Plus214_Output_0': array([-0.0429085 , -0.30849236, -1.1172674 ,  0.18745418,  0.26956522,\n",
       "         0.8740529 ,  0.04995521, -1.3036187 ,  1.2071588 ,  0.03463553],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = result.get()\n",
    "result # result is the value of the future - dict of np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queue Time: 0.00014897999999963218\n",
      "Compute Time: 0.00037888700000010544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0106 09:35:36.705937 16428 infer_runner.h:97] Execute Finished\n"
     ]
    }
   ],
   "source": [
    "with display_output():\n",
    "    start = time.process_time()\n",
    "    result = mnist.infer(**{k: np.random.random_sample(v['shape']) for k,v in mnist.input_bindings().items()})\n",
    "    print(\"Queue Time: {}\".format(time.process_time() - start))\n",
    "    result = result.get()\n",
    "    print(\"Compute Time: {}\".format(time.process_time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test for Correctness\n",
    "\n",
    "Load test image and results.  [Thanks to the ONNX Model Zoo](https://github.com/onnx/models/tree/master/mnist) for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = utils.load_inputs(\"/work/models/onnx/mnist-v1.3/test_data_set_0\")\n",
    "expected = utils.load_outputs(\"/work/models/onnx/mnist-v1.3/test_data_set_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  975.6701  ,  -618.72394 ,  6574.5684  ,   668.02893 ,\n",
       "         -917.27094 , -1671.6359  , -1952.7599  ,   -61.549873,\n",
       "         -777.17664 , -1439.5316  ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.mnist_image(inputs[0]).show()\n",
    "expected[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Submit the images to the inference queue, then wait for each result to be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [mnist.infer(Input3=input) for input in inputs]\n",
    "results = [r.get() for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results.\n",
    "TODO - update the utils to return dictionaries instead of arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Binding Name: Plus214_Output_0; shape: (10,)\n",
      "Result: 2\n"
     ]
    }
   ],
   "source": [
    "for r, e in zip(results, expected):\n",
    "    for key, val in r.items():\n",
    "        r = val.reshape((1,10))\n",
    "        np.testing.assert_almost_equal(r, e, decimal=3)\n",
    "        print(\"Output Binding Name: {}; shape: {}\".format(key, val.shape))\n",
    "        print(\"Result: {}\".format(np.argmax(utils.softmax(r))))\n",
    "        # r # show the raw tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
