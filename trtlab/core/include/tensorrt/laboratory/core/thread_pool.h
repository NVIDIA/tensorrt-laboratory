/* Copyright (c) 2018-2019, NVIDIA CORPORATION. All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 *  * Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 *  * Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *  * Neither the name of NVIDIA CORPORATION nor the names of its
 *    contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
 * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
 * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
 * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */
//
// Original Source: https://github.com/progschj/BaseThreadPool
//
// Original License:
//
// Copyright (c) 2012 Jakob Progsch, VÃ¡clav Zeman
//
// This software is provided 'as-is', without any express or implied
// warranty. In no event will the authors be held liable for any damages
// arising from the use of this software.
//
// Permission is granted to anyone to use this software for any purpose,
// including commercial applications, and to alter it and redistribute it
// freely, subject to the following restrictions:
//
//   1. The origin of this software must not be misrepresented; you must not
//   claim that you wrote the original software. If you use this software
//   in a product, an acknowledgment in the product documentation would be
//   appreciated but is not required.
//
//   2. Altered source versions must be plainly marked as such, and must not be
//   misrepresented as being the original software.
//
//   3. This notice may not be removed or altered from any source
//   distribution.
//
// Modifications:
//   * Header-only file was split into .h/.cc files
//   * Added an extra safety check (lines 30-31) in the construction (.cc file).
//   * Added CPU affinity options to the constructor
//   * Added Size() method to get thread count
//   * Implemented transwarp::executor protocol
//
#pragma once

#include "tensorrt/laboratory/core/affinity.h"
#include "tensorrt/laboratory/core/utils.h"

#include <future>
#include <queue>

#include <glog/logging.h>

namespace trtlab {

template<typename MutexType, typename ConditionType>
class BaseThreadPool;

using ThreadPool = BaseThreadPool<std::mutex, std::condition_variable>;

/**
 * @brief Manages a Pool of Threads that consume a shared work Queue
 *
 * BaseThreadPool is the primary resoruce class for handling threads used throughout
 * the YAIS examples and tests.  The library is entirely a BYO-resources;
 * however, this implemenation is provided as a convenience class.  Many thanks
 * to the original authors for a beautifully designed class.
 */
template<typename MutexType, typename ConditionType>
class BaseThreadPool
{
  public:
    /**
     * @brief Construct a new Thread Pool
     * @param nThreads Number of Worker Threads
     */
    BaseThreadPool(size_t nThreads);

    /**
     * @brief Construct a new Thread Pool with Shared CPU Affinity
     *
     * Creates a BaseThreadPool with nThreads where each thread sets its CPU affinity
     * to affinity_mask.
     *
     * @param nThreads
     * @param affinity_mask
     */
    BaseThreadPool(size_t nThreads, const CpuSet& affinity_mask);

    /**
     * @brief Construct a new Thread Pool with Exclusive CPU Affinity
     *
     * Creates a BaseThreadPool using a CpuSet such a for each CPU in the CpuSet a
     * thread is created and the affnity of that thread is dedicted to the
     * respective CPU from the CpuSet.
     *
     * @param cpu_set
     */
    BaseThreadPool(const CpuSet& cpu_set);
    virtual ~BaseThreadPool();

    DELETE_COPYABILITY(BaseThreadPool);
    DELETE_MOVEABILITY(BaseThreadPool);

    /**
     * @brief Enqueue Work to the BaseThreadPool by passing a Lambda Function
     *
     * Variadic template allows for an arbituary number of arguments to be passed
     * the captured lambda function.  Captures are still allowed and used
     * throughout the examples.
     *
     * The queue can grow larger than the number of threads.  A single worker
     * thread executues pulls a lambda function off the queue and executes it to
     * completion.  These are synchronous executions in an async messaging
     * library.  These synchronous pools can be swapped for truely async workers
     * using libevent or asio.  Happy to accept PRs to improve the async
     * abilities.
     *
     * @tparam F
     * @tparam Args
     * @param f
     * @param args
     * @return std::future<typename std::result_of<F(Args...)>::type>
     */
    template<class F, class... Args>
    auto enqueue(F&& f, Args&&... args) -> std::future<typename std::result_of<F(Args...)>::type>;

    /**
     * @brief Number of Threads in the Pool
     */
    int Size();

#ifdef USE_TRANSWARP
    // transwarp interface: get_name, execute

    // The name of the executor
    std::string get_name() const final override { return "trtlab::BaseThreadPool"; }

    // Only ever called on the thread of the caller to schedule()
    void execute(const std::function<void()>& functor,
                 const std::shared_ptr<tw::node>& node) final override
    {
        {
            std::unique_lock<MutexType> lock(m_QueueMutex);
            tasks.push(functor);
        }
        m_Condition.notify_one();
    }
#endif

  private:
    void CreateThread(const CpuSet& affinity_mask);

    // need to keep track of threads so we can join them
    std::vector<std::thread> workers;
    // the task queue
    std::queue<std::function<void()>> tasks;

    // synchronization
    MutexType m_QueueMutex;
    ConditionType m_Condition;
    bool stop;
};

// add new work item to the pool
template<typename MutexType, typename ConditionType>
template<class F, class... Args>
auto BaseThreadPool<MutexType, ConditionType>::enqueue(F&& f, Args&&... args)
    -> std::future<typename std::result_of<F(Args...)>::type>
{
    using return_type = typename std::result_of<F(Args...)>::type;

    auto task = std::make_shared<std::packaged_task<return_type()>>(
        std::bind(std::forward<F>(f), std::forward<Args>(args)...));

    std::future<return_type> res = task->get_future();
    {
        std::lock_guard<MutexType> lock(m_QueueMutex);

        // don't allow enqueueing after stopping the pool
        if(stop) throw std::runtime_error("enqueue on stopped BaseThreadPool");

        tasks.emplace([task]() { (*task)(); });
    }
    m_Condition.notify_one();
    return res;
}

template<typename MutexType, typename ConditionType>
BaseThreadPool<MutexType, ConditionType>::BaseThreadPool(size_t nThreads)
    : BaseThreadPool(nThreads, Affinity::GetAffinity())
{
}

template<typename MutexType, typename ConditionType>
BaseThreadPool<MutexType, ConditionType>::BaseThreadPool(size_t nThreads,
                                                         const CpuSet& affinity_mask)
    : stop(false)
{
    for(size_t i = 0; i < nThreads; ++i)
    {
        CreateThread(affinity_mask);
    }
}

template<typename MutexType, typename ConditionType>
BaseThreadPool<MutexType, ConditionType>::BaseThreadPool(const CpuSet& cpus) : stop(false)
{
    auto exclusive = cpus.GetAllocator();
    for(size_t i = 0; i < exclusive.size(); i++)
    {
        CpuSet affinity_mask;
        CHECK(exclusive.allocate(affinity_mask, 1)) << "Affinity Allocator failed on pass: " << i;
        CreateThread(affinity_mask);
    }
}

template<typename MutexType, typename ConditionType>
void BaseThreadPool<MutexType, ConditionType>::CreateThread(const CpuSet& affinity_mask)
{
    workers.emplace_back([this, affinity_mask]() {
        Affinity::SetAffinity(affinity_mask);
        DLOG(INFO) << "Initializing Thread " << std::this_thread::get_id() << " with CPU affinity "
                   << affinity_mask.GetCpuString();
        for(;;)
        {
            std::function<void()> task;
            {
                std::unique_lock<MutexType> lock(this->m_QueueMutex);
                this->m_Condition.wait(lock,
                                       [this]() { return this->stop || !this->tasks.empty(); });
                if(this->stop && this->tasks.empty()) return;
                task = move(this->tasks.front());
                this->tasks.pop();
            }
            task();
        }
    });
}

// the destructor joins all threads
template<typename MutexType, typename ConditionType>
BaseThreadPool<MutexType, ConditionType>::~BaseThreadPool()
{
    {
        std::lock_guard<MutexType> lock(m_QueueMutex);
        stop = true;
    }
    m_Condition.notify_all();

    for(std::thread& worker : workers)
    {
        worker.join();
    }
}

template<typename MutexType, typename ConditionType>
int BaseThreadPool<MutexType, ConditionType>::Size()
{
    return workers.size();
}

} // namespace trtlab
